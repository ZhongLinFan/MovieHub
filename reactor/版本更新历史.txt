2023年7月6日15点05分：基础服务器部分写完，v0.1,其对应的功能主要有线程池，任务队列，reactor多反应堆（底层检测模型有三个），读写buf，日志输出，主要功能是能接收任意长度大小的数据并回显，客户端使用linux的nc工具，已打包（当前版本发现的问题没有解决的：就是关掉客户端，服务器端会连续打印）

V0.2
2023年7月6日16点01分：添加保存客户端IP和端口的功能
17点34分，增加了接收客户端失败时的错误分流
19点47分 预计增加内存池

2023年7月7日13点16分由于，构建内存池需要链表节点有指针，所以打算重构Buffer，也就是Buffer为父类，子类为IObuf，父类中有指针，子类中，子类中只关注读写事件，不过由于会很大程度改动Buf的类，而昨天在进行错误分流时，改了不少buf的东西，，如果直接改，会丢失一些比较好的改动点，还是在改这个Buf之前打包成V0.15

2023年7月7日18点39分 内存池测试完毕，v0.2完成

20点50分 消息封装，消息解析函数，连接属性（tcpserver里里记录所有的连接），服务器主动发消息函数，资源释放机制，客户端接口（因为后面客户端直接new出来就能用写的所有东西了，而且要用到那里面之前的所有的类，所以可以看成一个框架提供两个接口，一个server，一个client）v0.3
2023年7月11日凌晨完成v0.3并测试完成，主要功能是客户端和服务器进行互动（你发完，我接收完，然后回显。。。。）不断的随机发送字符串，最终没有出现异常情况，没有大问题，有一个小bug是客户端检测到服务器断开之后无法释放资源，这个我还没处理，可以后期再处理，其他没啥问题

2023年7月11日13点27分 V0.4版本 增加事件路由机制，简单Hook机制
2023年7月12日09点48分 0.4版本完成，并且完善了客户端的资源释放机制，修复了0.3版本没有发现的两个主要bug，可以看相关记录

2023年7月12日09点50分V0.5 增加消息队列的普通任务，udp协议支持 protobuf协议支持，增加性能测试qps，增加连接属性外部传递信息设置，给tcpserver提供一些简单易用的接口，接口先不提供了，等有需求再更改吧
2023年7月13日20点53分v0.5完成

2023年7月13日20点56分基于上述框架实现一个Dns服务器框架（包含数据库连接池）
11点44分查阅了一些资料，在线影院项目的架构思想是
1、linux下的qt做外壳（服务器和客户端全都是在linux下进行的，因为服务区和客户端使用的是一个静态库，只支持linux）
2、在线影院项目包含架构层包括：负载均衡服务器（可能有聊天的，音视频的等）他是负责将聊天业务等平均分摊到指定的提供对应服务的服务器，业务分发模型（将当前业务分配到对应的负责相对应业务的负载均衡服务器上）
3、业务主要包括在线聊天业务，流媒体业务，基于流媒体包装的，在线观影、在线视频，远程控制
4、现阶段所要完成的东西：
	1、基于reactor库的：负载均衡服务器（这个其实是流量分发）
	2、业务分发模型（这个其实也是流量分发，只不过是根据业务分的，这个模型里存放了固定的ip地址和端口，比如业务序号为1，会对应一条ip和端口，这个端口是负载均衡的服务器地址，然后这个负载均衡服务器（负载均衡服务器存放了对应服务器的ip地址，当然了这个负载均衡服务器的设计后面有时间就可以各个负载均衡服务器的对应关系表也是存放在数据库中的，这里负载思路就是先集中到同一个服务器，当超过上限值，再分配到其他服务器，当然 了如果是不同房间，那肯定要分配到其他服务器，后期有时间就可以结果为导向的，进行分配）会根据服务器的当前分配情况把这个业务分分配给一个流媒体服务器（如果是同一部电影尽量分配到同一个流媒体服务器上，因为如果分配到不同的服务器上，那么在响应的时候，两台服务器都要请求数据库，然后访问远程的文件，这样会增加服务器的负担，如果是同一个数据，那么完全可以访问一次数据库，从文件系统读取一次，然后让其他线程推流），当然了同一个流媒体服务器也应该有上限值，上限值超过了应该需要把他分配到另一台服务器上）（这里业务分发模型是客户端，负载均衡服务器是服务器）
	3、在线聊天室，（这个的负载均衡服务器思路也是类似的，因为我是转发消息，为了防止大量的拷贝，可以一个房间尽量安排在同一个服务器上）
（这里有一个问题需要解决，就是对端肯定只和服务器的一个主机申请cfd，也就是服务器这边最终只有一个cfd，那聊天业务怎么办呢，也就是聊天业务没用cfd可用，这就需要客户端登录的时候同时申请建立多个业务的cfd，那么问题又来了，那怎么同步呢，就是如果下线了怎么通知另一台服务器呢，这个就可以通过聊天服务器告诉负载均衡服务器再告诉业务分发服务器，由业务分发服务器来管理）
（有一个很严重的问题就是客户端启动之后应该先发一个包给业务分发服务器，请求两个建立（视频流，和文字聊天流，还有视频通话（这个应该是动态的）），这个业务分发模型应该返回两个IP地址（业务分发模型可以向均衡负载申请），然后第二个包再请求建立，这样好像就不需要业务分发模型了，不过还是需要的，因为客户端不知道各个服务器的集群）
（针对上面的问题，或者可以将cfd直接连接到业务服务器上，同时，业务服务器和各个业务服务器建立连接？？？这样好像并不可行吧，那肯定还要和负载均衡服务器建立连接，这不可取。。。而且转发流的过程更耗时，一台业务服务器肯定不行，所以这里肯定不能进行）
	4、流媒体服务器，（这个可以将ffplay移植过来）
	5、基于流媒体服务器的在线观影，
	6、在线视频，
	7、远程控制，
	8、数据库连接池，（这个的作用是干啥的，如果流媒体服务器需要读取资源，因为是集群，文件不可能存在每个集群中的流媒体服务器上，所以需要一个专门存放文件的地方，这个时候问题来了，怎么获取文件在哪个地方，这个时候应该是业务分发模型把要播放的视频文件名称发给流媒体服务器包装的在线观影服务器，，然后此时由在线观影服务器的单独的一个线程去数据库查询到地址，然后这个线程紧接着去文件路径那里读取数据到一块共享内存，然后通知对应的客户端列表去读取这里面的数据，读取完之后继续读剩余的流，直到读完之后停止读取，然后通知对应的cfd发送消息到对端播放完成。注意这个读的过程需要一个定时器，因为可能有的度的快，有的读的慢，我需要保证一秒这个共享内存被写入25次，不管你有没有读完。）（数据库模块是服务于在线观影服务器的）、		9、定时器模块（这个应该集成到reactor）
	10、用户管理模块（包括登录功能）、（数据库应该有两张表，一张是资源表（存放当前的资源路径），一张是用户表，用户表存放用户名，用户id，播放历史，朋友列表等）
	11、判断下线逻辑，需要心跳，如果长时间没反应，那么需要认为是下线了，然后主动给断开连接


经过上面的分析下一步实现一个负载均衡服务器，和数据库连接池和在线聊天服务器和反向代理服务器

但是今天又意识到一个问题，之前的第一代架构是客户端->业务分发模型->若干个负载均衡服务器->若干服务器->数据库和存储，工作大概过程是业务分发模型接收所有的请求，然后根据业务类型进行一次分流，分流到对应的负载均衡服务器上，然后负载均衡服务器负责挑选一个服务器处理这个请求，但是后面发现，在客户端第一次请求的时候，就应该将客户端和对应的业务服务器建立一条连接，也就是服务器和客户端内存里都应该有一个fd对应，但是前面的设想中客户端发过来一个连接请求给业务分发模型时，业务分发模型就应该accept，然后业务分发模型在把请求发到负载均衡服务器的时候，是不是也要accept，相当于业务分发模型和负载均衡服务器为这条请求建立了一条连接，然后是负载均衡服务器再和服务器为这个客户端建立一条连接，这显然是不合理的。
第二个版本就是业务分发模型只负责第一次请求的包，然后业务分发模型去对应的负载均衡服务器请求一个服务器地址返回给客户端，然后客户端去根据这个地址建立连接，注意这里的负载均衡服务器还是多个，
第三个版本是：这个负载均衡服务器完全可以用一台就可以呀，大不了增加一个业务号，根据这个业务号去查找对应的主机，这时候突然意识到业务分发模型完全没必要存在了，因为只有一台负载均衡服务器了，客户端可以直接请求负载均衡服务器
第四个版本是：如果增加一个服务器，那是不是要重新编译，应该把对应的地址写到对应的文件中（这个也要重启服务器，肯定也接收不了，如果每次读，其实也可以吧）或者数据库中，去数据库进行查找
第五个版本是：如果某台服务器出现宕机，那么按照上面的思想就检查不出来，客户端应该增加一个机制，就是每次请求完，返回一个结果包，对当前请求做出评价，然后负载均衡服务器把对应的结果写到数据库或者文件中以便下次选取时进行参考
第六个版本是：客户端在第一次请求ip地址的时候需要建立连接吗，完全可以用udp去发一个包，这样就不用每次建立一个连接了，然后建立完再销毁了

上面的假想是客户端和服务器的后续业务都是tcp连接的，那如果是udp呢，上面的模型需要调整吗，不需要调整，因为客户端发起udp请求之前也是要获得对方的ip地址的，如果是客户端给第一代的业务分发模型发送请求，然后由业务分发模型去交给对应的负载均衡服务器，然后负载均衡服务器再转交给对应的服务器，完全没必要，我为啥不在一开始就把地址指定到对应的服务器上呢，这样我就不用在中间加一层去强行的走我这个服务器，所以上面的模型对于udp也是成熟的，而且由于是取流（一台客户端的取流过程应该是对应一台服务器的，而不应该一台客户端在这边取一段流，在另一台服务器上去一段流，显然不合理，因为取流的状态是需要连续的）
第7个版本是如果项目够大，应该再增加多级的负载均衡服务器，类似于dns
第8个版本是如果多个业务之间进行状态同步怎么办，不太好举例子，应该是服务器1把状态变化的信息反馈到负载均衡服务器上，然后由负载均衡服务器发给对应的另一台服务器2上，然后服务器根据收到的包执行一段函数，更新自己的状态。


经过上面的分析下一步实现一个负载均衡服务器，而负载均衡服务器需要使用udp，那么先实现使用udp的服务器和客户端
2023年7月15日15点12分：这是reactor的第6个版本：实现udp服务器和客户端的接口
增加了udpserver和client，并且修复了客户端使用内存池不合理的现象，并且修改了Buffer读写udp的方法，增加了udp路由类，增加了message对于udp的支持，有很多重复的代码了，后期有时间可以重新优化
2023年7月16日21点28分完成了udp服务器和客户端的接口，并且极大优化了头文件包含的问题，主要理解了前置声明的用途和怎么在一个类里使用另一个类里using的包装器

2023年7月16日21点31分，实现一个连接池，供负载均衡服务器从数据库中查找表和流媒体服务器查找文件，这个应该也需要放到reactor里面

2023年7月18日17点36分，完成连接池并进行测试，发现效果惊人，可以看0.6的测试用例

2023年7月18日写数据库代理服务器，但是发现在写数据库代理服务器时，完全可以把之前写的数据库连接池，放到mysqlAgent文件中，这里完全没必要参杂，因为即使这里编译了，上层在创建数据库对象的时候，makefile还是要包含mysq的lib库和头文件，所以在reactor这里完全没必要包含mysql之类的东西（之前要包含是因为当时任务每个服务器都应该有一个连接池，而不是把所有的连接池都放到一起，抽象出来），所以把0.7版本的数据库相关文件移动到mysqlAgent里，这可能会照成linux端和win端的0.7版本文件不一致，因为linux端并未删除（已经删除了第7版的包，并将linux和win端的mysql数据放到对应的文件夹），所以最终reactor只有0.6个版本，而是把第7版的打包成了mysqlAgentV0.1版本